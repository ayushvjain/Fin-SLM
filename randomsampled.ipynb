{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11300058,"sourceType":"datasetVersion","datasetId":7066530}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T17:21:16.821093Z","iopub.execute_input":"2025-04-07T17:21:16.821316Z","iopub.status.idle":"2025-04-07T17:21:18.057126Z","shell.execute_reply.started":"2025-04-07T17:21:16.821296Z","shell.execute_reply":"2025-04-07T17:21:18.055948Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/text-data/tokenized_output.txt\n/kaggle/input/text-data/financial_tokenizer.model\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport sentencepiece as spm\nfrom torch.utils.data import Dataset\nimport numpy as np\n\nclass FinancialDataset(Dataset):\n    def __init__(self, tokenized_txt_path, tokenizer_path, max_len=512):\n        self.tokenized_txt_path = tokenized_txt_path\n        self.tokenizer = spm.SentencePieceProcessor(model_file=tokenizer_path)\n        self.max_len = max_len\n        self.data = self.load_data()\n\n    def load_data(self):\n        with open(self.tokenized_txt_path, 'r') as f:\n            text = f.read().split('\\n')\n        sentences = [line.strip() for line in text if line.strip() and not line.startswith('---')]\n        return sentences\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        sentence = self.data[idx]\n        \n        # Encode the sentence into tokens (input sequence)\n        input_ids = self.tokenizer.encode(sentence)\n        \n        # Pad the input sequence to max_len\n        if len(input_ids) < self.max_len:\n            padding_length = self.max_len - len(input_ids)\n            input_ids = input_ids + [0] * padding_length  # Pad with 0s\n        \n        # Truncate to max_len\n        input_ids = input_ids[:self.max_len]\n        \n        # Create input and target sequences (target sequence is the same as input shifted by 1)\n        input_tensor = torch.tensor(input_ids[:-1], dtype=torch.long)  # All tokens except the last one for input\n        target_tensor = torch.tensor(input_ids[1:], dtype=torch.long)  # All tokens except the first one for target\n        \n        return input_tensor, target_tensor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T17:26:44.489641Z","iopub.execute_input":"2025-04-07T17:26:44.489938Z","iopub.status.idle":"2025-04-07T17:26:48.093065Z","shell.execute_reply.started":"2025-04-07T17:26:44.489918Z","shell.execute_reply":"2025-04-07T17:26:48.092207Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torch\n\ndef collate_fn(batch):\n    \"\"\"\n    Custom collate function to pad sequences in a batch to the same length.\n    \"\"\"\n    input_tensors, target_tensors = zip(*batch)\n    \n    # Pad sequences to the maximum length in the batch\n    input_padded = torch.nn.utils.rnn.pad_sequence(input_tensors, batch_first=True, padding_value=0)\n    target_padded = torch.nn.utils.rnn.pad_sequence(target_tensors, batch_first=True, padding_value=0)\n    \n    return input_padded, target_padded\n\n# Initialize dataset and dataloader with the custom collate_fn\ndataset = FinancialDataset(tokenized_txt_path='/kaggle/input/text-data/tokenized_output.txt', tokenizer_path='/kaggle/input/text-data/financial_tokenizer.model', max_len=512)\ndataloader = DataLoader(dataset, batch_size=8, collate_fn=collate_fn, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T17:27:39.683872Z","iopub.execute_input":"2025-04-07T17:27:39.684340Z","iopub.status.idle":"2025-04-07T17:27:43.027684Z","shell.execute_reply.started":"2025-04-07T17:27:39.684312Z","shell.execute_reply":"2025-04-07T17:27:43.027038Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch.nn as nn\nclass BiLSTMModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim=300, hidden_dim=512, num_layers=2):\n        super(BiLSTMModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n\n    def forward(self, x):\n        embedded = self.embedding(x)                # (batch_size, seq_len, embedding_dim)\n        lstm_out, _ = self.lstm(embedded)           # (batch_size, seq_len, hidden_dim*2)\n        output = self.fc(lstm_out)                  # (batch_size, seq_len, vocab_size)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T17:28:40.427963Z","iopub.execute_input":"2025-04-07T17:28:40.428329Z","iopub.status.idle":"2025-04-07T17:28:40.433281Z","shell.execute_reply.started":"2025-04-07T17:28:40.428304Z","shell.execute_reply":"2025-04-07T17:28:40.432529Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\nimport torch.optim as optim\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyperparameters\nvocab_size = len(dataset.tokenizer)\nembedding_dim = 300\nhidden_dim = 512\n\n# Model, optimizer, loss\nmodel = BiLSTMModel(vocab_size, embedding_dim, hidden_dim).to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Load checkpoint if it exists\nstart_epoch = 0\ncheckpoint_path = \"bilstm_checkpoint.pth\"\n\nif os.path.exists(checkpoint_path):\n    print(\"Loading checkpoint...\")\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch'] + 1\n    print(f\"Resuming from epoch {start_epoch}\")\n\n# Training loop\nnum_epochs = 3\nfor epoch in range(start_epoch, num_epochs):\n    model.train()\n    total_loss = 0\n\n    for input_tensor, target_tensor in dataloader:\n        input_tensor = input_tensor.to(device)\n        target_tensor = target_tensor.to(device)\n\n        optimizer.zero_grad()\n        output = model(input_tensor)\n        loss = criterion(output.view(-1, vocab_size), target_tensor.view(-1))\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(dataloader)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n\n    # Save checkpoint\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': avg_loss,\n    }, checkpoint_path)\n    print(f\"Checkpoint saved at epoch {epoch+1}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T22:52:15.159137Z","iopub.execute_input":"2025-04-07T22:52:15.159357Z","iopub.status.idle":"2025-04-07T22:52:15.347031Z","shell.execute_reply.started":"2025-04-07T22:52:15.159338Z","shell.execute_reply":"2025-04-07T22:52:15.346170Z"}},"outputs":[{"name":"stdout","text":"Loading checkpoint...\nResuming from epoch 3\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-8-3105e4257e56>:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path, map_location=device)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import math\n\ndef calculate_perplexity(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n\n    with torch.no_grad():\n        for input_tensor, target_tensor in dataloader:\n            input_tensor = input_tensor.to(device)\n            target_tensor = target_tensor.to(device)\n\n            output = model(input_tensor)  # (batch, seq_len, vocab_size)\n            loss = criterion(output.view(-1, vocab_size), target_tensor.view(-1))\n            \n            total_loss += loss.item() * target_tensor.numel()  # scale by number of tokens\n            total_tokens += target_tensor.numel()\n\n    average_loss = total_loss / total_tokens\n    perplexity = math.exp(average_loss)\n    return perplexity\n\n# Usage\nperplexity = calculate_perplexity(model, dataloader, criterion, device)\nprint(f\"Perplexity: {perplexity:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T23:45:33.850712Z","iopub.execute_input":"2025-04-07T23:45:33.850896Z","iopub.status.idle":"2025-04-08T00:34:36.107842Z","shell.execute_reply.started":"2025-04-07T23:45:33.850879Z","shell.execute_reply":"2025-04-08T00:34:36.106949Z"}},"outputs":[{"name":"stdout","text":"Perplexity: 1.0000\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def generate_text(model, tokenizer, prompt, max_new_tokens=50, device='cpu', temperature=1.0):\n    model.eval()\n    \n    # Encode the prompt\n    input_ids = tokenizer.encode(prompt)\n    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n\n    generated = input_ids[:]\n    \n    for _ in range(max_new_tokens):\n        input_tensor = torch.tensor(generated[-512:], dtype=torch.long).unsqueeze(0).to(device)\n        with torch.no_grad():\n            output = model(input_tensor)\n        \n        logits = output[0, -1, :] / temperature  # Get logits for the last token\n        probabilities = torch.softmax(logits, dim=-1)\n        next_token = torch.multinomial(probabilities, num_samples=1).item()\n\n        generated.append(next_token)\n\n        # If tokenizer has an end-of-sentence token, you can break on that.\n        # For example:\n        # if next_token == tokenizer.eos_id():\n        #     break\n\n    return tokenizer.decode(generated)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T00:48:39.979457Z","iopub.execute_input":"2025-04-08T00:48:39.979732Z","iopub.status.idle":"2025-04-08T00:48:39.985234Z","shell.execute_reply.started":"2025-04-08T00:48:39.979712Z","shell.execute_reply":"2025-04-08T00:48:39.984319Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"prompt = \"what are the latest global trends in finance\"\ngenerated_text = generate_text(model, dataset.tokenizer, prompt, max_new_tokens=50, device=device)\nprint(\"\\nGenerated text:\\n\")\nprint(generated_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T00:48:52.272040Z","iopub.execute_input":"2025-04-08T00:48:52.272352Z","iopub.status.idle":"2025-04-08T00:48:53.044312Z","shell.execute_reply.started":"2025-04-08T00:48:52.272330Z","shell.execute_reply":"2025-04-08T00:48:53.043569Z"}},"outputs":[{"name":"stdout","text":"\nGenerated text:\n\nwhat are the latest global trends in finance                                                  \n","output_type":"stream"}],"execution_count":13}]}