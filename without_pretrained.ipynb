{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11300058,"sourceType":"datasetVersion","datasetId":7066530}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport sentencepiece as spm\nfrom torch.utils.data import Dataset\nimport numpy as np\n\nclass FinancialDataset(Dataset):\n    def __init__(self, tokenized_txt_path, tokenizer_path, max_len=512):\n        self.tokenized_txt_path = tokenized_txt_path\n        self.tokenizer = spm.SentencePieceProcessor(model_file=tokenizer_path)\n        self.max_len = max_len\n        self.data = self.load_data()\n\n    def load_data(self):\n        with open(self.tokenized_txt_path, 'r') as f:\n            text = f.read().split('\\n')\n        sentences = [line.strip() for line in text if line.strip() and not line.startswith('---')]\n        return sentences\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        sentence = self.data[idx]\n        \n        # Encode the sentence into tokens (input sequence)\n        input_ids = self.tokenizer.encode(sentence)\n        \n        # Pad the input sequence to max_len\n        if len(input_ids) < self.max_len:\n            padding_length = self.max_len - len(input_ids)\n            input_ids = input_ids + [0] * padding_length  # Pad with 0s\n        \n        # Truncate to max_len\n        input_ids = input_ids[:self.max_len]\n        \n        # Create input and target sequences (target sequence is the same as input shifted by 1)\n        input_tensor = torch.tensor(input_ids[:-1], dtype=torch.long)  # All tokens except the last one for input\n        target_tensor = torch.tensor(input_ids[1:], dtype=torch.long)  # All tokens except the first one for target\n        \n        return input_tensor, target_tensor\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torch\n\ndef collate_fn(batch):\n    \"\"\"\n    Custom collate function to pad sequences in a batch to the same length.\n    \"\"\"\n    input_tensors, target_tensors = zip(*batch)\n    \n    # Pad sequences to the maximum length in the batch\n    input_padded = torch.nn.utils.rnn.pad_sequence(input_tensors, batch_first=True, padding_value=0)\n    target_padded = torch.nn.utils.rnn.pad_sequence(target_tensors, batch_first=True, padding_value=0)\n    \n    return input_padded, target_padded\n\n# Initialize dataset and dataloader with the custom collate_fn\ndataset = FinancialDataset(tokenized_txt_path='/kaggle/input/text-data/tokenized_output.txt', tokenizer_path='/kaggle/input/text-data/financial_tokenizer.model', max_len=512)\ndataloader = DataLoader(dataset, batch_size=8, collate_fn=collate_fn, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:05:01.785147Z","iopub.execute_input":"2025-04-07T01:05:01.785426Z","iopub.status.idle":"2025-04-07T01:05:03.726100Z","shell.execute_reply.started":"2025-04-07T01:05:01.785405Z","shell.execute_reply":"2025-04-07T01:05:03.725370Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Check if CUDA (GPU) is available, if not, use CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Define BiLSTM model (as in previous code)\nclass BiLSTMModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2):\n        super(BiLSTMModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, vocab_size)  # *2 for bidirectional\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        lstm_out, (hidden, cell) = self.lstm(embedded)\n        out = self.fc(lstm_out)\n        return out\n\n# Hyperparameters\nembedding_dim = 256\nhidden_dim = 512\nvocab_size = len(dataset.tokenizer)  # Size of the vocabulary from SentencePiece model\nmodel = BiLSTMModel(vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_dim=hidden_dim).to(device)\n\n# Optimizer and Loss Function\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Training Loop\nnum_epochs = 3\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for input_tensor, target_tensor in dataloader:\n        # Move tensors to the same device (either GPU or CPU)\n        input_tensor, target_tensor = input_tensor.to(device), target_tensor.to(device)\n\n        optimizer.zero_grad()\n        output = model(input_tensor)\n        # Loss computation (only consider the tokenized sequence part)\n        loss = criterion(output.view(-1, vocab_size), target_tensor.view(-1))\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        print('Current epoch',str(epoch),'Loss',str(total_loss))\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\n\ndef calculate_perplexity(model, dataloader):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for input_tensor, target_tensor in dataloader:\n            # Move tensors to GPU if available\n            input_tensor, target_tensor = input_tensor.cuda(), target_tensor.cuda()\n\n            output = model(input_tensor)\n            loss = criterion(output.view(-1, vocab_size), target_tensor.view(-1))\n            total_loss += loss.item()\n    \n    avg_loss = total_loss / len(dataloader)\n    perplexity = math.exp(avg_loss)\n    return perplexity\n\n# After training\nperplexity = calculate_perplexity(model, dataloader)\nprint(f\"Perplexity: {perplexity}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}